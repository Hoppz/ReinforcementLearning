{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class MCAgent(object):\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 0 up, 1 right ,\n",
    "env = gym.make('CliffWalking-v0')"
   ],
   "id": "61a264b7b1f0a18a",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T02:20:20.930198Z",
     "start_time": "2024-05-30T02:20:20.873438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/envs python \n",
    "# -*- coding:utf-8 -*-\n",
    "import time\n",
    " \n",
    "import numpy as np\n",
    "import gym\n",
    " \n",
    " \n",
    "class SarsaAgent(object):\n",
    "    def __init__(self, obs_n, act_n, learning_rate=0.01, gamma=0.9, e_greedy=0.1):\n",
    "        self.act_n = act_n       # 动作的维度， 有几个动作可选\n",
    "        self.lr = learning_rate    # 学习率\n",
    "        self.gamma = gamma   # 折扣因子，reward的衰减率\n",
    "        self.epsilon = e_greedy   # 按一定的概率随机选动作\n",
    "        self.Q = np.zeros((obs_n, act_n))   # 创建一个Q表格\n",
    " \n",
    " \n",
    "    # 根据输入观察值（这个代码不区分state和observation），采样输出的动作值\n",
    "    def sample(self, obs):\n",
    "        if np.random.uniform(0, 1) < (1.0 - self.epsilon):    # 根据table的Q值选动作\n",
    "            action = self.predict(obs)    # 调用函数获得要在该观察值（或状态）条件下要执行的动作\n",
    "        else:\n",
    "            action = np.random.choice(self.act_n)   # e_greedy概率直接从动作空间中随机选取一个动作\n",
    "        return action\n",
    " \n",
    " \n",
    "    # 根据输入的观察值，预测输出的动作值\n",
    "    def predict(self, obs):\n",
    "        Q_list = self.Q[obs, :]    # 从Q表中选取状态(或观察值)对应的那一行\n",
    "        maxQ = np.max(Q_list)    # 获取这一行最大的Q值，可能出现多个相同的最大值\n",
    " \n",
    "        action_list = np.where(Q_list == maxQ)[0]    # np.where(条件)功能是筛选出满足条件的元素的坐标\n",
    "        action = np.random.choice(action_list)      # 这里尤其如果最大值出现了多次，随机取一个最大值对应的动作就成\n",
    "        return action\n",
    " \n",
    "    # 给环境作用一个动作后，对环境的所有反馈进行学习，也就是用环境反馈的结果来更新Q-table\n",
    "    def learn(self, obs, action, reward, next_obs, next_action, done):\n",
    "        \"\"\"\n",
    "            on-policy\n",
    "            obs：交互前的obs, 这里observation和state通用，也就是公式或者伪代码码中的s_t\n",
    "            action： 本次交互选择的动作， 也就是公式或者伪代码中的a_t\n",
    "            reward: 本次与环境交互后的奖励,  也就是公式或者伪代码中的r\n",
    "            next_obs: 本次交互环境返回的下一个状态，也就是s_t+1\n",
    "            next_action: 根据当前的Q表，针对next_obs会选择的动作，a_t+1\n",
    "            done: 回合episode是否结束\n",
    "        \"\"\"\n",
    "        predict_Q = self.Q[obs, action]\n",
    "        if done:\n",
    "            target_Q = reward     # 如果到达终止状态， 没有下一个状态了，直接把奖励赋值给target_Q\n",
    "        else:\n",
    "            target_Q = reward + self.gamma * self.Q[next_obs, next_action]      # 这两行代码直接看伪代码或者公式\n",
    "        self.Q[obs, action] = predict_Q + self.lr * (target_Q - predict_Q)      # 修正q\n",
    " \n",
    " \n",
    " \n",
    "def run_episode(env, agent, render=False):\n",
    "    total_steps = 0    # 记录每一个回合episode走了多少step\n",
    "    total_reward = 0    # 记录一个episode获得总奖励\n",
    " \n",
    "    obs = env.reset()   # 重置环境，重新开始新的一轮（episode)\n",
    "    action = agent.sample(obs)    # 根据算法选择一个动作，采用ε-贪婪算法选取动作\n",
    " \n",
    "    while True:\n",
    "        next_obs, reward, done, info = env.step(action)    # 与环境进行一次交互，即把动作action作用到环境，并得到环境的反馈\n",
    "        next_action = agent.sample(next_obs)   # 根据获得的下一个状态，执行ε-贪婪算法后，获得下一个动作\n",
    " \n",
    "        # 训练Sarsa算法， 更新Q表格\n",
    "        agent.learn(obs, action, reward, next_obs, next_action, done)\n",
    " \n",
    "        action = next_action\n",
    "        obs = next_obs   # 存储上一个观测值（这里状态和观测不区分，正常observation是state的一部分）\n",
    " \n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    " \n",
    "        if render:\n",
    "            env.render()     # 重新画一份效果图\n",
    "        if done:      # 如果达到了终止状态，则回合结束，跳出该轮循环\n",
    "            break\n",
    "    return total_reward, total_steps\n",
    " \n",
    " \n",
    " \n",
    "def test_episode(env, agent):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    " \n",
    "    while True:\n",
    "        action = agent.predict(obs)  # greedy\n",
    "        next_obs, reward, done, info = env.step(action)  # 执行一步，这是强化学习最关键的一行代码，通俗讲就是智能体采取这个动作action，环境就相应的发生了变化，得到下一个状态next_obs, 奖励reward, 是否回合结束done(True or False)，其它信息info\n",
    "        total_reward += reward\n",
    "        obs = next_obs\n",
    "        time.sleep(0.5)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print('test reward = %.lf' % (total_reward))\n",
    " \n",
    "def main():\n",
    "    env = gym.make(\"CliffWalking-v0\")   # 悬崖边行走游戏，动作空间及其表示为：0 up , 1 right, 2 down, 3 left\n",
    " \n",
    "    agent = SarsaAgent(\n",
    "        obs_n=env.observation_space.n,\n",
    "        act_n=env.action_space.n,\n",
    "        learning_rate=0.1,\n",
    "        gamma=0.9,\n",
    "        e_greedy=0.1)\n",
    " \n",
    "    is_render = False\n",
    " \n",
    "    for episode in range(500):\n",
    "        ep_reward, ep_steps = run_episode(env, agent, is_render)\n",
    "        print('Episode %s: steps = %s, reward = %.lf' % (episode, ep_steps, ep_reward))\n",
    " \n",
    " \n",
    "        # 每隔20个episode渲染一下看看效果\n",
    "        if episode % 20 == 0:\n",
    "            is_render = True\n",
    "        else:\n",
    "            is_render = False\n",
    " \n",
    " \n",
    "    # 训练结束，查看算法效果\n",
    "    test_episode(env, agent)\n",
    " \n",
    " \n",
    "\n",
    " \n",
    "main()"
   ],
   "id": "9edea5981f00f356",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "975ee1fd376c4f2f",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
